{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_Keras Regression and Classification.ipynb","provenance":[],"collapsed_sections":["d364Zs8XXfvo"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-jMcj5lSXftV","colab_type":"text"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"180\" /></center>\n","\n","<br> \n","\n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/image/keras-logo-2018-large-1200.png\" width=\"600\" height=\"200\" /></center>\n","                                                                                                           \n","                                                                                                          "]},{"cell_type":"markdown","metadata":{"id":"0jTUjTD6XftX","colab_type":"text"},"source":["### Table of Content\n","\n","\n","1. [Linear Regression in Keras](#section1)<br>\n","2. [Logistic Regression in Keras](#section2)<br>\n","3. [Multiclass Logistic Regression in Keras](#section3)<br>\n"]},{"cell_type":"markdown","metadata":{"id":"jt7lIzIRXftY","colab_type":"text"},"source":["<a id=section1></a>"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"asPo91gnXfta","colab_type":"text"},"source":["## 1. Linear Regression in Keras"]},{"cell_type":"markdown","metadata":{"id":"M2CaAAOdXftc","colab_type":"text"},"source":["<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/image/linear_regression.png\" style=\"width: 600px;\"/>                                                                                              \n","Despite the implications of the above image, linear regression is quite simple. As we have already studied, linear regression is useful for __finding relationship between two continuous variables__. One is __predictor or independent variable__ and other is __response or dependent variable__.\n","\n","The core idea is to obtain a __line that best fits the data__. The best fit line is the one for which __total prediction error (all data points) is as small as possible__. Error is the distance between the point to the regression line.\n","\n","Let's try basic regression in Keras - "]},{"cell_type":"markdown","metadata":{"id":"-PA1ZcdkXftd","colab_type":"text"},"source":["### 1. Import the Libraries\n","* NumPy is the omnipresent library for machine learning in Python.\n","* Keras has a default Sequential model which refers to a linear stack of layers. In our current example, we only need the one hidden layer.\n","* A dense layer, as explained previously has each of its neurons connected to every neuron in the previous layer.\n","* We also need `matplotlib` for plotting out our prediction."]},{"cell_type":"code","metadata":{"id":"ItkIWd-QuyJn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1579503309628,"user_tz":-330,"elapsed":1943,"user":{"displayName":"Mohit Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWeTZUrbhhCiZ-3HbrA91YVJTlW3olUgVSeOr8Ww=s64","userId":"17943522262909727589"}},"outputId":"b56cdedf-e2a7-42a4-f80b-bbcfd4143a33"},"source":["# Import tensorflow 2.x\n","# This code block will only work in Google Colab.\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uMdpiV0DAuP6","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"wfMlGfn1Xftf","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cV3HMCP7Xftk","colab_type":"text"},"source":["### 2. Prepare the Data\n","We use random seeds to generate data within the range (-1,1)."]},{"cell_type":"code","metadata":{"id":"cNM3R-P_Xftl","colab_type":"code","colab":{}},"source":["np.random.seed(1337)                                                # just a random number seed for reproducibility\n","X = np.linspace(-1, 1, 200)                                         # create 200 x values between [-1,1]\n","np.random.shuffle(X)                                                # randomize the data\n","y = 2 * X + np.random.randn(*X.shape) * 0.33                        # create 200 y values randomized by the seed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qC0VFtvBXftq","colab_type":"text"},"source":["### 3. Plot the Data\n","We use matplotlib to __check how our data is distributed.__ This gives us a good sense of how to approach the problem. For now, we are sure to use linear regression, but __visualizing the data is the key__ to applying an algorithm successfully."]},{"cell_type":"code","metadata":{"id":"m2iuU4LfXfts","colab_type":"code","colab":{},"outputId":"1f9190ce-dedc-451e-da4a-b3e905897c20"},"source":["# plot data\n","plt.scatter(X, y)\n","plt.xlabel('X')\n","plt.ylabel('Y')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIgcDAhERASAAYGIiBwMCEREBAD4/yH9nl6QzuiBAAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x2446c6a47b8>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Uokno9kaXftz","colab_type":"text"},"source":["We have our initial data points. We have used a __relatively simple distribution__ for explanatory purposes."]},{"cell_type":"markdown","metadata":{"id":"jO-Na2HaXft0","colab_type":"text"},"source":["### 4. Split the Data\n","Now we __split the data into train and test sets.__ This helps in __validating our model.__ If we check the accuracy of a model on a training set, it is analogous to cheating for an exam. This gives us a good idea of how well our model would actually perform on a real dataset."]},{"cell_type":"code","metadata":{"id":"-Uqi8MUuXft2","colab_type":"code","colab":{}},"source":["# train test split\n","X_train, y_train = X[:160], y[:160]     #split train:test in 160:40\n","X_test, y_test = X[160:], y[160:]  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pi-avg3FXft7","colab_type":"text"},"source":["### 5. Create a Neural Network Model\n","Now we create a basic neural network model using Keras' convenient syntax. We will see the details of the parameters in the Neural Network module. The focus here is how to do regression with *Keras*."]},{"cell_type":"code","metadata":{"id":"IPbebz9FXft8","colab_type":"code","colab":{}},"source":["model = Sequential() # Sequential -> linear stack of layers.\n","model.add(Dense(input_dim=1, units=1, kernel_initializer='uniform', activation='linear')) # we will study about \n","# layer weight initializers and activations in a later session"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hXB2zlgcXfuE","colab_type":"text"},"source":["__After designing__ our model, we __compile__ it. This basically means __doing forward and backward propagation iterations__. Keras handles this just with the __single line__."]},{"cell_type":"code","metadata":{"id":"eeNUtLdbXfuG","colab_type":"code","colab":{}},"source":["model.compile(loss='mse', optimizer='sgd') # loss is mean squared error, and sgd refers to Stochastic Gradient Descent.\n","# these parametric details will be studied in the Neural Network session."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LW0e0fAqXfuM","colab_type":"text"},"source":["Getting the initial weights and biases for later comparison - "]},{"cell_type":"code","metadata":{"id":"_nymT-sAXfuO","colab_type":"code","colab":{},"outputId":"5cb4a7dc-81d0-4204-c175-e97bd9031cce"},"source":["# Print initial weights\n","weights = model.layers[0].get_weights()\n","w_init = weights[0][0][0]\n","b_init = weights[1][0]\n","print('Linear regression model is initialized with weight w: %.2f, b: %.2f' % (w_init, b_init))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Linear regression model is initialized with weight w: 0.02, b: 0.00\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EjDNNFEQXfuU","colab_type":"text"},"source":["### 6. Train the Model\n","Training the model can be done in one of the following two ways - \n","1. Train the model in a single line using `model.fit()`\n","2. Train manually by feeding the batches step-by-step"]},{"cell_type":"code","metadata":{"id":"jQG6LnHhXfuV","colab_type":"code","colab":{},"outputId":"d5871b1d-1337-4f3a-cfcd-e5c097e80358"},"source":["# First way\n","model.fit(X_train, y_train, epochs=100, verbose=1) \n","# Epochs are the number of iterations for training. Verbose can be switched to 0 if you don't want to print \n","# the step-wise training loss. Try it.\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","160/160 [==============================] - 0s 3ms/step - loss: 1.3486\n","Epoch 2/100\n","160/160 [==============================] - 0s 62us/step - loss: 1.2670\n","Epoch 3/100\n","160/160 [==============================] - 0s 56us/step - loss: 1.1911\n","Epoch 4/100\n","160/160 [==============================] - 0s 56us/step - loss: 1.1196\n","Epoch 5/100\n","160/160 [==============================] - 0s 87us/step - loss: 1.0531\n","Epoch 6/100\n","160/160 [==============================] - 0s 87us/step - loss: 0.9909\n","Epoch 7/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.9327\n","Epoch 8/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.8786\n","Epoch 9/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.8282\n","Epoch 10/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.7806\n","Epoch 11/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.7367\n","Epoch 12/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.6950\n","Epoch 13/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.6567\n","Epoch 14/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.6211\n","Epoch 15/100\n","160/160 [==============================] - 0s 50us/step - loss: 0.5876\n","Epoch 16/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.5559\n","Epoch 17/100\n","160/160 [==============================] - 0s 94us/step - loss: 0.5268\n","Epoch 18/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4995\n","Epoch 19/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4745\n","Epoch 20/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.4504\n","Epoch 21/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4280\n","Epoch 22/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.4073\n","Epoch 23/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.3878\n","Epoch 24/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.3696\n","Epoch 25/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.3529\n","Epoch 26/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.3370\n","Epoch 27/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.3222\n","Epoch 28/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.3086\n","Epoch 29/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2955\n","Epoch 30/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.2837\n","Epoch 31/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2723\n","Epoch 32/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2620\n","Epoch 33/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.2521\n","Epoch 34/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2429\n","Epoch 35/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.2345\n","Epoch 36/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2265\n","Epoch 37/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2191\n","Epoch 38/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2122\n","Epoch 39/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.2056\n","Epoch 40/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1996\n","Epoch 41/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1942\n","Epoch 42/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1886\n","Epoch 43/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1837\n","Epoch 44/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.1791\n","Epoch 45/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1748\n","Epoch 46/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1708\n","Epoch 47/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1671\n","Epoch 48/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1636\n","Epoch 49/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1603\n","Epoch 50/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1573\n","Epoch 51/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1545\n","Epoch 52/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1518\n","Epoch 53/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1493\n","Epoch 54/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1470\n","Epoch 55/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1447\n","Epoch 56/100\n","160/160 [==============================] - 0s 50us/step - loss: 0.1428\n","Epoch 57/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.1409\n","Epoch 58/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1391\n","Epoch 59/100\n","160/160 [==============================] - 0s 50us/step - loss: 0.1375\n","Epoch 60/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1360\n","Epoch 61/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1345\n","Epoch 62/100\n","160/160 [==============================] - 0s 50us/step - loss: 0.1332\n","Epoch 63/100\n","160/160 [==============================] - 0s 112us/step - loss: 0.1319\n","Epoch 64/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1307\n","Epoch 65/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1296\n","Epoch 66/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1286\n","Epoch 67/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1277\n","Epoch 68/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1268\n","Epoch 69/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1259\n","Epoch 70/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.1252\n","Epoch 71/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1244\n","Epoch 72/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1237\n","Epoch 73/100\n","160/160 [==============================] - 0s 50us/step - loss: 0.1231\n","Epoch 74/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1226\n","Epoch 75/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1220\n","Epoch 76/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1215\n","Epoch 77/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1210\n","Epoch 78/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1206\n","Epoch 79/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1202\n","Epoch 80/100\n","160/160 [==============================] - 0s 50us/step - loss: 0.1197\n","Epoch 81/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1194\n","Epoch 82/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1191\n","Epoch 83/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1187\n","Epoch 84/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1184\n","Epoch 85/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1182\n","Epoch 86/100\n","160/160 [==============================] - 0s 50us/step - loss: 0.1180\n","Epoch 87/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1177\n","Epoch 88/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1175\n","Epoch 89/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1174\n","Epoch 90/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1170\n","Epoch 91/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1169\n","Epoch 92/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1167\n","Epoch 93/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1165\n","Epoch 94/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1163\n","Epoch 95/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1163\n","Epoch 96/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1161\n","Epoch 97/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1160\n","Epoch 98/100\n","160/160 [==============================] - 0s 87us/step - loss: 0.1159\n","Epoch 99/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1157\n","Epoch 100/100\n","160/160 [==============================] - 0s 94us/step - loss: 0.1157\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x2446c7856d8>"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"nAb9xuO3Xfua","colab_type":"text"},"source":["A __good indication of how well the model performs__ can be seen as the __loss/cost decreases.__ \n","\n","Note that __if you have executed the above code block once__ in the notebook, the model is defined and its parameters are set, meaning that the __loss will already be at the minimum.__ This is why the second way below has its loss already minimized. You can __refresh the notebook to retrain the model from start.__"]},{"cell_type":"code","metadata":{"id":"VfBZe1FkXfuc","colab_type":"code","colab":{},"outputId":"18151ff2-28d3-413a-dc60-0d6b1e6feff0"},"source":["# Second way\n","for step in range(101):\n","    cost = model.train_on_batch(X_train, y_train)\n","    if step % 10 == 0:\n","        print('train cost: ', cost)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train cost:  0.11452594\n","train cost:  0.114482865\n","train cost:  0.11444531\n","train cost:  0.11441257\n","train cost:  0.11438401\n","train cost:  0.11435912\n","train cost:  0.1143374\n","train cost:  0.11431847\n","train cost:  0.114301965\n","train cost:  0.114287555\n","train cost:  0.11427499\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"47jp9mIOXfug","colab_type":"text"},"source":["### 7. Evaluate the Model\n","\n","Now we evaluate our model on the test data. A __low cost indicates a good convergence__ of the model. Notice how the weights and biases vary from their initial values."]},{"cell_type":"code","metadata":{"id":"SMmqZxOxXfui","colab_type":"code","colab":{},"outputId":"9e665e50-58dd-4744-9ad0-9d8dc734429f"},"source":["print('\\nTesting ------------')\n","cost = model.evaluate(X_test, y_test, batch_size=40)\n","print('test cost:', cost)\n","W, b = model.layers[0].get_weights()\n","print('Weights=', W, '\\nbiases=', b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Testing ------------\n","40/40 [==============================] - 0s 50us/step\n","test cost: 0.14142563939094543\n","Weights= [[1.9424988]] \n","biases= [0.02676049]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OhNLEMJvXfuq","colab_type":"text"},"source":["### 8. Plot our Prediction (Optional)\n","We already know our model is performing well. But we want to *see* it. Enter matplotlib.\n","To plot our predictions vs the initial weights, we utilize the graph again."]},{"cell_type":"code","metadata":{"id":"YG77kwPZXfuq","colab_type":"code","colab":{},"outputId":"0f5252e2-7f97-436f-fca6-7460cc3d4b48"},"source":["y_pred = model.predict(X_test)\n","plt.scatter(X_test, y_test)\n","plt.plot(X_test, y_pred)\n","plt.plot(X_train, w_init*X_train + b_init, label='initial')\n","plt.plot(X_test, w*X_test + b, label='prediction')\n","plt.xlabel('X')\n","plt.ylabel('Y')\n","plt.title(\"Linear Regression\")\n","plt.legend()\n","plt.show()\n","# Bonus points for noticing the tri-colors"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKSGIiAighCAiIr7/D90OfqK8U3U9AAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x1244cfd4ac8>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"CmmClpjAXfuu","colab_type":"text"},"source":["That is it for linear regression using Keras. Now on to classification using logistic regression."]},{"cell_type":"markdown","metadata":{"id":"YUOSs04nXfuv","colab_type":"text"},"source":["<a id=section2></a>"]},{"cell_type":"markdown","metadata":{"id":"5gWlHwmUXfuw","colab_type":"text"},"source":["## 2. Logistic Regression in Keras"]},{"cell_type":"markdown","metadata":{"id":"E6AUPj6uXfux","colab_type":"text"},"source":["<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/image/Logistic-Regression-1-Neuron-1200x682.png\" style=\"width: 600px;\"/>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y77vvcyOXfuy","colab_type":"text"},"source":["Logistic Regression differs from Linear in the way that we have to __classify instead of predicting a value__. In the above image, we see that the classifier built is a Dog/Not-Dog classifier. \n","\n","Logistic Regression is used when the __dependent variable(target) is categorical__.\n","\n","These are examples of __binary logistic regression__. For __more than two classes__, we have a similar concept, instead we just call it __multinomial/multivariate logistic regression.__\n","`N` output nodes in a Logistic Regression network can be used to predict `N` different classes.\n","\n","Let's try binomial Logistic Regression.\n","We use similar libraries for logistic regression as well - "]},{"cell_type":"markdown","metadata":{"id":"5VYERuXrXfu1","colab_type":"text"},"source":["### 1. Importing the libraries\n","We use __Keras for the neural network modelling__ with __sklearn for preprocessing the data__ as well as __matplotlib for visualizing the data.__"]},{"cell_type":"code","metadata":{"id":"RL2BN-zaA0Ut","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras import optimizers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BitkM7v6Xfu5","colab_type":"code","colab":{},"outputId":"6b8e3edf-d13a-488e-935c-0e8c22f42c78"},"source":["from sklearn.datasets.samples_generator import make_blobs\n","from sklearn.preprocessing import MinMaxScaler"],"execution_count":null,"outputs":[{"output_type":"stream","text":["c:\\users\\gdev\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n","  from ._conv import register_converters as _register_converters\n","Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BAjRPtVRBHB1","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TI8o8n_8XfvA","colab_type":"text"},"source":["### 2. Generating the dataset -\n","We use sklearn to __make blobs or random samples of data__ centred around two points for ease of visualization. We then __normalize/scale the input__ to better fit the model.\n","\n","Also, we __split the data into train and test set__ to check the performance of our model later on."]},{"cell_type":"code","metadata":{"id":"gSo_Vhi-XfvB","colab_type":"code","colab":{},"outputId":"6e2da9c7-c8e7-4c80-a5bc-2d7201ccf66a"},"source":["X, y = make_blobs(n_samples=200, centers=2, n_features=2, random_state=1)\n","scalar = MinMaxScaler()\n","scalar.fit(X)\n","X = scalar.transform(X)\n","\n","# train test split\n","X_train, y_train = X[:160], y[:160]     #split train:test in 160:40\n","X_test, y_test = X[160:], y[160:] "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0\n"," 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1\n"," 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0\n"," 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0\n"," 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n"," 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8JnhIshgXfvH","colab_type":"text"},"source":["Let's see the data we created on the graph - "]},{"cell_type":"code","metadata":{"id":"bte47Gk5XfvJ","colab_type":"code","colab":{},"outputId":"3258cda2-ea7c-404b-b037-ca6a53f37afe"},"source":["plt.scatter(X[:,0],X[:,1])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x24464bc9128>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"F2jLOsx7XfvO","colab_type":"text"},"source":["### 3. Define the model\n","Now for the crux of the algorithm, we __define our neural network model.__ It takes __2 dimensions/features as input__ and gives a __single output.__ In-between we have __two hidden layers with 4 hidden units each.__ Our model looks like this ->\n","\n","*[2 input nodes] -> [4 nodes hidden1] -> [4 nodes hidden2] -> [single node output]*"]},{"cell_type":"code","metadata":{"id":"empAPUsrXfvP","colab_type":"code","colab":{},"outputId":"caaa695e-0143-4f13-85e2-60216caf5221"},"source":["model = Sequential()\n","model.add(Dense(4, input_dim=2, activation='relu'))\n","model.add(Dense(4, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_4 (Dense)              (None, 4)                 12        \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 4)                 20        \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 1)                 5         \n","=================================================================\n","Total params: 37\n","Trainable params: 37\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9g8h0E2DXfvT","colab_type":"text"},"source":["Note that in the above __model summary, we don't see the input layer as it is counted as the 0th layer.__\n","\n","Detailed description for activation functions, loss and optimizers will be covered later. For now, let's focus on implementing the code with Keras.\n","\n","Let's check the weights and biases that have been initialized - "]},{"cell_type":"code","metadata":{"id":"kQoRoa2FXfvU","colab_type":"code","colab":{},"outputId":"2fd3e572-9076-4d85-a0e0-c432584dd3b4"},"source":["# Print initial weights\n","weights = model.layers[0].get_weights()\n","w_init = weights[0]\n","b_init = weights[1]\n","print(\"Logistic regression model is initialized with weights - {} and biases - {}\".format(w_init, b_init))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Logistic regression model is initialized with weights - [[-0.9199724   0.8810086   0.98212314  0.10879111]\n"," [-0.3191085  -0.7047384   0.9874644  -0.5125544 ]] and biases - [0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Vra-_orXfvZ","colab_type":"text"},"source":["### 4. Train the Neural Network\n","Keras takes care of __training the model with just a single line.__ Since we specified the __metrics as 'accuracy'__ above, we can see the accuracy increasing with each epoch. This is one of the biggest advantages of using Keras over raw TensorFlow.\n"]},{"cell_type":"code","metadata":{"id":"GN7hoTcPXfvZ","colab_type":"code","colab":{},"outputId":"44819ca0-c33a-4ee8-bef7-fcb7ca7265b7"},"source":["history = model.fit(X_train, y_train, epochs=100, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","160/160 [==============================] - 0s 3ms/step - loss: 0.5795 - acc: 0.4937\n","Epoch 2/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.5747 - acc: 0.4937\n","Epoch 3/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.5701 - acc: 0.4937\n","Epoch 4/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.5653 - acc: 0.5000\n","Epoch 5/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.5607 - acc: 0.5000\n","Epoch 6/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.5562 - acc: 0.5062\n","Epoch 7/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.5515 - acc: 0.5062\n","Epoch 8/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.5469 - acc: 0.5062\n","Epoch 9/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.5422 - acc: 0.5062\n","Epoch 10/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.5376 - acc: 0.5062\n","Epoch 11/100\n","160/160 [==============================] - 0s 50us/step - loss: 0.5329 - acc: 0.5125\n","Epoch 12/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.5283 - acc: 0.5125\n","Epoch 13/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.5236 - acc: 0.5187\n","Epoch 14/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.5190 - acc: 0.5187\n","Epoch 15/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.5143 - acc: 0.5187\n","Epoch 16/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.5096 - acc: 0.5312\n","Epoch 17/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.5050 - acc: 0.5500\n","Epoch 18/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.5002 - acc: 0.5563\n","Epoch 19/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4954 - acc: 0.5750\n","Epoch 20/100\n","160/160 [==============================] - 0s 106us/step - loss: 0.4906 - acc: 0.6063\n","Epoch 21/100\n","160/160 [==============================] - 0s 87us/step - loss: 0.4859 - acc: 0.6250\n","Epoch 22/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4811 - acc: 0.6500\n","Epoch 23/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.4763 - acc: 0.6625\n","Epoch 24/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.4714 - acc: 0.6875\n","Epoch 25/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4666 - acc: 0.7188\n","Epoch 26/100\n","160/160 [==============================] - 0s 87us/step - loss: 0.4617 - acc: 0.7562\n","Epoch 27/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.4567 - acc: 0.7625\n","Epoch 28/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.4517 - acc: 0.7812\n","Epoch 29/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.4466 - acc: 0.8000\n","Epoch 30/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4416 - acc: 0.8187\n","Epoch 31/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.4362 - acc: 0.8312\n","Epoch 32/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4308 - acc: 0.8500\n","Epoch 33/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.4253 - acc: 0.8750\n","Epoch 34/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4195 - acc: 0.8812\n","Epoch 35/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.4137 - acc: 0.9125\n","Epoch 36/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.4082 - acc: 0.9250\n","Epoch 37/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.4022 - acc: 0.9437\n","Epoch 38/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.3964 - acc: 0.9500\n","Epoch 39/100\n","160/160 [==============================] - 0s 93us/step - loss: 0.3907 - acc: 0.9562\n","Epoch 40/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.3851 - acc: 0.9562\n","Epoch 41/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.3795 - acc: 0.9625\n","Epoch 42/100\n","160/160 [==============================] - 0s 87us/step - loss: 0.3739 - acc: 0.9688\n","Epoch 43/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.3685 - acc: 0.9688\n","Epoch 44/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.3633 - acc: 0.9750\n","Epoch 45/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.3582 - acc: 0.9813\n","Epoch 46/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.3534 - acc: 0.9875\n","Epoch 47/100\n","160/160 [==============================] - 0s 106us/step - loss: 0.3488 - acc: 0.9875\n","Epoch 48/100\n","160/160 [==============================] - 0s 100us/step - loss: 0.3440 - acc: 0.9875\n","Epoch 49/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.3394 - acc: 0.9938\n","Epoch 50/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.3349 - acc: 0.9938\n","Epoch 51/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.3304 - acc: 0.9938\n","Epoch 52/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.3260 - acc: 0.9938\n","Epoch 53/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.3216 - acc: 0.9938\n","Epoch 54/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.3172 - acc: 0.9938\n","Epoch 55/100\n","160/160 [==============================] - 0s 93us/step - loss: 0.3129 - acc: 0.9938\n","Epoch 56/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.3086 - acc: 0.9938\n","Epoch 57/100\n","160/160 [==============================] - 0s 106us/step - loss: 0.3045 - acc: 0.9938\n","Epoch 58/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.3003 - acc: 0.9938\n","Epoch 59/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2961 - acc: 0.9938\n","Epoch 60/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2921 - acc: 0.9938\n","Epoch 61/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.2881 - acc: 0.9938\n","Epoch 62/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2842 - acc: 0.9938\n","Epoch 63/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.2803 - acc: 0.9938\n","Epoch 64/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.2764 - acc: 0.9938\n","Epoch 65/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.2726 - acc: 0.9938\n","Epoch 66/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2688 - acc: 0.9938\n","Epoch 67/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.2649 - acc: 0.9938\n","Epoch 68/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.2611 - acc: 0.9938\n","Epoch 69/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.2575 - acc: 0.9938\n","Epoch 70/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.2538 - acc: 0.9938\n","Epoch 71/100\n","160/160 [==============================] - 0s 87us/step - loss: 0.2500 - acc: 0.9938\n","Epoch 72/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2465 - acc: 0.9938\n","Epoch 73/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.2428 - acc: 0.9938\n","Epoch 74/100\n","160/160 [==============================] - 0s 87us/step - loss: 0.2391 - acc: 0.9938\n","Epoch 75/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2355 - acc: 0.9938\n","Epoch 76/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.2320 - acc: 0.9938\n","Epoch 77/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.2285 - acc: 0.9938\n","Epoch 78/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2250 - acc: 1.0000\n","Epoch 79/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.2215 - acc: 1.0000\n","Epoch 80/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2180 - acc: 1.0000\n","Epoch 81/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2146 - acc: 1.0000\n","Epoch 82/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.2112 - acc: 1.0000\n","Epoch 83/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.2079 - acc: 1.0000\n","Epoch 84/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.2044 - acc: 1.0000\n","Epoch 85/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.2011 - acc: 1.0000\n","Epoch 86/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.1977 - acc: 1.0000\n","Epoch 87/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1944 - acc: 1.0000\n","Epoch 88/100\n","160/160 [==============================] - 0s 56us/step - loss: 0.1910 - acc: 1.0000\n","Epoch 89/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1877 - acc: 1.0000\n","Epoch 90/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1844 - acc: 1.0000\n","Epoch 91/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1811 - acc: 1.0000\n","Epoch 92/100\n","160/160 [==============================] - 0s 62us/step - loss: 0.1779 - acc: 1.0000\n","Epoch 93/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.1748 - acc: 1.0000\n","Epoch 94/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1716 - acc: 1.0000\n","Epoch 95/100\n","160/160 [==============================] - 0s 81us/step - loss: 0.1684 - acc: 1.0000\n","Epoch 96/100\n","160/160 [==============================] - 0s 87us/step - loss: 0.1653 - acc: 1.0000\n","Epoch 97/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1622 - acc: 1.0000\n","Epoch 98/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1591 - acc: 1.0000\n","Epoch 99/100\n","160/160 [==============================] - 0s 75us/step - loss: 0.1561 - acc: 1.0000\n","Epoch 100/100\n","160/160 [==============================] - 0s 69us/step - loss: 0.1531 - acc: 1.0000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zNq-y_tYXfvc","colab_type":"text"},"source":["### 5. Evaluate the Model\n","\n","Finally, we may also want to see how our weights and biases have been updated along with the decrease in cost - "]},{"cell_type":"code","metadata":{"id":"1wWglyJkXfvf","colab_type":"code","colab":{},"outputId":"414079d5-de2b-4725-d2e6-ba57c77904e2"},"source":["print('\\nTesting ------------')\n","cost = model.evaluate(X_test, y_test, batch_size=40)\n","print('test cost:', cost)\n","weights = model.layers[0].get_weights()\n","W = weights[0]\n","b = weights[1]\n","print('Weights=', W, '\\nbiases=', b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Testing ------------\n","40/40 [==============================] - 0s 4ms/step\n","test cost: [0.6906261444091797, 0.6000000238418579]\n","Weights= [[-0.23040056 -0.56944084  0.5315294   0.0026679 ]\n"," [ 0.5058191  -0.75729895 -0.16045666 -0.8928783 ]] \n","biases= [0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HnzEo_W3Xfvl","colab_type":"text"},"source":["That's it for binomial logistic regression with Keras. Let's do more than two classes now - "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ORggFxuyXfvl","colab_type":"text"},"source":["<a id=section3></a>"]},{"cell_type":"markdown","metadata":{"id":"Igmb5DhPXfvl","colab_type":"text"},"source":["## 3. Multiclass Logistic Regression in Keras"]},{"cell_type":"markdown","metadata":{"id":"fEiI5XtfXfvm","colab_type":"text"},"source":["<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/image/20120612-omnomnomnivore-dinosaurs-post.jpg\" style=\"width: 400px;\"/>\n","\n","                                                                                                      "]},{"cell_type":"markdown","metadata":{"id":"yZTqKmz7Xfvn","colab_type":"text"},"source":["Multiclass classification in Keras works almost similarly as in TensorFlow, with the difference that Keras handles a lot of boilerplate code on its own. Let's take a look."]},{"cell_type":"markdown","metadata":{"id":"d364Zs8XXfvo","colab_type":"text"},"source":["### 0. Problem Description\n","In this tutorial, we will again use the standard machine learning iris flowers dataset.\n","\n","<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/image/iris.jpeg\" style=\"width: 600px;\"/>\n","\n","                                                                                                        "]},{"cell_type":"markdown","metadata":{"id":"z93hBWumXfvp","colab_type":"text"},"source":["### 1. Import Classes and Functions\n","* We begin by importing the __functionality we require from Keras__, __data loading from pandas__ as well as __data preparation and model evaluation from scikit-learn.__\n","\n","* Again, we __download the Iris dataset__ using python as shown below into a file we name raw.csv."]},{"cell_type":"code","metadata":{"id":"3_bdiWxHBDLs","colab_type":"code","colab":{}},"source":["import numpy\n","import pandas\n","import requests\n","import re\n","import seaborn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGuYnO3_BLXd","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.pipeline import Pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFk_SZ1pBNq0","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYlqOVOyXfvq","colab_type":"code","colab":{},"outputId":"2f589826-8731-4e81-97f1-e7f641117efe"},"source":["# Download the dataset\n","url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n","r = requests.get(url, allow_redirects=True)\n","filename = \"raw.csv\"\n","open(filename, 'wb').write(r.content)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4551"]},"metadata":{"tags":[]},"execution_count":202}]},{"cell_type":"markdown","metadata":{"id":"xmP7vqPHXfvt","colab_type":"text"},"source":["### 2. Initialize Random Number Generator\n","Next, we need to __initialize the random number generator__ to a constant value (7). This will come handy later on for shuffling our dataset."]},{"cell_type":"code","metadata":{"id":"G_2-y3KIXfvu","colab_type":"code","colab":{}},"source":["# fix random seed for reproducibility\n","seed = 7\n","numpy.random.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Iz7Q8n_Xfvw","colab_type":"text"},"source":["### 3. Load The Dataset\n","The dataset can be loaded directly. Because the output variable contains strings, it is __easiest to load the data using pandas.__ We can then __split the attributes (columns) into input variables (X) and output variables (Y).__"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"pMWXzTphXfvx","colab_type":"code","colab":{},"outputId":"660073d4-8f24-472d-8902-9e7a1d97e2c8"},"source":["#load the dataset into memory\n","dataset = pandas.read_csv('raw.csv', header=None, names=['sepal_length','sepal_width','petal_length','petal_width','species'])\n","dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sepal_length  sepal_width  petal_length  petal_width      species\n","0           5.1          3.5           1.4          0.2  Iris-setosa\n","1           4.9          3.0           1.4          0.2  Iris-setosa\n","2           4.7          3.2           1.3          0.2  Iris-setosa\n","3           4.6          3.1           1.5          0.2  Iris-setosa\n","4           5.0          3.6           1.4          0.2  Iris-setosa"]},"metadata":{"tags":[]},"execution_count":204}]},{"cell_type":"markdown","metadata":{"id":"FKs6MDcKXfv0","colab_type":"text"},"source":["### 4. Encode The Output Variable\n","* The output variable contains three different string values.\n","* We have already seen how we do categorical encoding for varibles in the TensorFlow notebook. Again, for multiclassification, we create dummy variables from a categorical variable."]},{"cell_type":"code","metadata":{"id":"TMf72ykgXfv0","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelBinarizer\n","species_lb = LabelBinarizer()\n","dummy_y = species_lb.fit_transform(dataset.species.values)\n","\n","from sklearn.preprocessing import normalize\n","FEATURES = dataset.columns[0:4]\n","X_data = dataset[FEATURES].as_matrix()\n","X = normalize(X_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bXxYMtWmXfv8","colab_type":"text"},"source":["### 5. Define The Neural Network Model\n","* The Keras library provides __wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.__\n","\n","* The network topology of this simple one-layer neural network can be summarized as: \n","\n","*4 inputs -> [8 hidden nodes] -> 3 outputs*"]},{"cell_type":"code","metadata":{"id":"FQAxKFAmXfv8","colab_type":"code","colab":{}},"source":["def baseline_model():\n","    # define baseline model\n","    model = Sequential()\n","    model.add(Dense(8, input_dim=4, activation='relu'))\n","    model.add(Dense(3, activation='softmax'))\n","    # Compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"trnaH-FtXfwA","colab_type":"code","colab":{}},"source":["estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vwlvWiTXfwD","colab_type":"text"},"source":["### 6. Evaluate The Model with k-Fold Cross Validation (Data Partitioning)\n","* The scikit-learn has excellent capability to evaluate models using a suite of techniques. The __gold standard for evaluating machine learning models is k-fold cross validation.__\n","\n","* Here, we set the number of folds to be 10 and to shuffle the data before partitioning it."]},{"cell_type":"code","metadata":{"id":"wLNu7gH7XfwI","colab_type":"code","colab":{}},"source":["kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1dnYYd7HXfwL","colab_type":"text"},"source":["Evaluating the model returns an object that describes the evaluation of the 10 constructed models for each of the 10 different splits of the dataset."]},{"cell_type":"code","metadata":{"id":"e1nSkop3XfwM","colab_type":"code","colab":{},"outputId":"818c175b-78d2-49ad-f7cd-6a0a20ea03e1"},"source":["results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n","print(\"Baseline accuracy: %.2f%% Standard deviation - (%.2f%%)\" % (results.mean()*100, results.std()*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Baseline accuracy: 96.67% Standard deviation - (4.47%)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EiHPmtW3XfwW","colab_type":"text"},"source":["### 7. Observing Performance\n","* We know that the __probability prediction for binary classification is defined as__\n","\n","    $y = 1/(1+e^{-(b0+b1X)})$\n","    \n","\n","* __For multiclass classification__, we can define the __probability as__\n","\n","    $y = 1/(1+e^{-(b0+b1X1+b2X2+....)})$\n","* Here, b1, b2, b3, b4 are sepal length, width, petal length and width respectively. b0 is the bias which we add separately.\n","* Let us take an example of the first example flower and watch how these features affect this probability - "]},{"cell_type":"code","metadata":{"scrolled":true,"id":"JdCiyEnAXfwX","colab_type":"code","colab":{},"outputId":"9f31de79-06b7-468a-8a87-1665cbc37728"},"source":["import numpy as np\n","print (X[0],dummy_y[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.80377277 0.55160877 0.22064351 0.0315205 ] [1 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MUDh98fUXfwZ","colab_type":"text"},"source":["Note that the y output for this is the __one-hot encoded array *[1 0 0]* indicating this flower is actually *iris-Setosa*.__\n","\n","Now let's check what our model predicts for this flower - "]},{"cell_type":"code","metadata":{"scrolled":true,"id":"a31_aVZ8Xfwa","colab_type":"code","colab":{},"outputId":"11e15f41-bc79-420c-a38f-a6513cd98f99"},"source":["predictions = model.predict(X)\n","print(predictions[0])\n","print(np.argmax(predictions[0])+1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.36169004 0.3089772  0.32933268]\n","1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zJv-zPIXXfwd","colab_type":"text"},"source":["Our model gives the __highest probability to the first flower, i.e. iris-Setosa, which is correct.__\n","\n","Now let's __modify the first input feature gradually to see how the prediction changes__ -"]},{"cell_type":"code","metadata":{"id":"oOMBU-TfXfwe","colab_type":"code","colab":{},"outputId":"f71500a6-b8bd-4306-aaa3-d213141f7f9c"},"source":["X_copy = np.copy(X)                                                          # We don't want to alter original dataset\n","dummy_sepal_length = 0.1\n","print(\"                Input Features                   One-Hot Label        Model Probability Prediction       Class Prediction\")\n","for i in range(10):\n","    X_copy[0][0] = i * dummy_sepal_length\n","    #print (X[136],dummy_y[136])\n","    predictions = model.predict(X_copy)\n","    print(X_copy[0], \"      \", dummy_y[0], \"      \", predictions[0],\"      \", np.argmax(predictions[0])+1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                Input Features                   One-Hot Label        Model Probability Prediction       Class Prediction\n","[0.         0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.3518693  0.33144593 0.3166848 ]        1\n","[0.1        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.34940583 0.32133323 0.32926095]        1\n","[0.2        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.35016426 0.31771496 0.33212078]        1\n","[0.3        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.3520631  0.31626484 0.33167204]        1\n","[0.4        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.35396603 0.31481588 0.33121812]        1\n","[0.5        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.355873   0.31336802 0.330759  ]        1\n","[0.6        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.35778394 0.31192133 0.3302947 ]        1\n","[0.7        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.3596988  0.31047586 0.32982525]        1\n","[0.8        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.3616176 0.3090317 0.3293507]        1\n","[0.9        0.55160877 0.22064351 0.0315205 ]        [1 0 0]        [0.36354017 0.3075888  0.328871  ]        1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ICMzGehgXfwo","colab_type":"text"},"source":["* Observe how the __first input feature changes with each iteration.__\n","* As we __increase sepal length, the probability prediction for the 1st species increases__, while it decreases for others.\n","* This indicates that __higher sepal length is associated with the 1st species i.e. Setosa.__\n","* Now let's try it with each of the other input features - "]},{"cell_type":"code","metadata":{"id":"2j7TEu0HXfwp","colab_type":"code","colab":{},"outputId":"f5718374-1bd6-4cbd-eaa0-0a96fab9cab9"},"source":["X_copy = np.copy(X)                                                          # We don't want to alter original dataset\n","dummy_sepal_width = 0.1\n","print(\"                Input Features                   One-Hot Label        Model Probability Prediction       Class Prediction\")\n","for i in range(10):\n","    X_copy[0][1] = i * dummy_sepal_length\n","    #print (X[136],dummy_y[136])\n","    predictions = model.predict(X_copy)\n","    print(X_copy[0], \"      \", dummy_y[0], \"      \", predictions[0],\"      \", np.argmax(predictions[0])+1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                Input Features                   One-Hot Label        Model Probability Prediction       Class Prediction\n","[0.80377277 0.         0.22064351 0.0315205 ]        [1 0 0]        [0.33040413 0.29648072 0.37311515]        3\n","[0.80377277 0.1        0.22064351 0.0315205 ]        [1 0 0]        [0.33872813 0.2999654  0.3613065 ]        3\n","[0.80377277 0.2        0.22064351 0.0315205 ]        [1 0 0]        [0.345915   0.30379283 0.3502922 ]        3\n","[0.80377277 0.3        0.22064351 0.0315205 ]        [1 0 0]        [0.3530647 0.3075039 0.3394314]        1\n","[0.80377277 0.4        0.22064351 0.0315205 ]        [1 0 0]        [0.35887676 0.3109337  0.3301895 ]        1\n","[0.80377277 0.5        0.22064351 0.0315205 ]        [1 0 0]        [0.3607557  0.30994356 0.32930067]        1\n","[0.80377277 0.6        0.22064351 0.0315205 ]        [1 0 0]        [0.36256656 0.30807233 0.3293611 ]        1\n","[0.80377277 0.7        0.22064351 0.0315205 ]        [1 0 0]        [0.36437902 0.30620614 0.3294148 ]        1\n","[0.80377277 0.8        0.22064351 0.0315205 ]        [1 0 0]        [0.36619315 0.30434507 0.32946184]        1\n","[0.80377277 0.9        0.22064351 0.0315205 ]        [1 0 0]        [0.36800876 0.30248913 0.32950217]        1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OV0J1E8YXfwx","colab_type":"text"},"source":["* Here, As we increase sepal width, the probability prediction for the 1st species increases, while it decreases for others.\n","* This indicates that __lower sepal width corresponds to 3rd species(Virginica), while higher sepal width is associated with the 1st species i.e. Setosa.__"]},{"cell_type":"code","metadata":{"id":"qUz9n9rQXfw4","colab_type":"code","colab":{},"outputId":"f2b1f385-ec34-4527-eb34-e9259b8ee403"},"source":["X_copy = np.copy(X)                                                          # We don't want to alter original dataset\n","dummy_petal_length = 0.01\n","print(\"                Input Features                   One-Hot Label        Model Probability Prediction       Class Prediction\")\n","for i in range(10):\n","    X_copy[136][1] = i * dummy_petal_length\n","    #print (X[136],dummy_y[136])\n","    predictions = model.predict(X_copy)\n","    print(X_copy[136], \"      \", dummy_y[136], \"      \", predictions[136],\"      \", np.argmax(predictions[136])+1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                Input Features                   One-Hot Label        Model Probability Prediction       Class Prediction\n","[0.67017484 0.         0.59571097 0.2553047 ]        [0 0 1]        [0.3448743  0.28679362 0.36833206]        3\n","[0.67017484 0.01       0.59571097 0.2553047 ]        [0 0 1]        [0.34551528 0.28749153 0.36699316]        3\n","[0.67017484 0.02       0.59571097 0.2553047 ]        [0 0 1]        [0.3461548  0.2881889  0.36565632]        3\n","[0.67017484 0.03       0.59571097 0.2553047 ]        [0 0 1]        [0.3467928 0.2888857 0.3643215]        3\n","[0.67017484 0.04       0.59571097 0.2553047 ]        [0 0 1]        [0.34742925 0.289582   0.36298874]        3\n","[0.67017484 0.05       0.59571097 0.2553047 ]        [0 0 1]        [0.3480642  0.29027772 0.36165807]        3\n","[0.67017484 0.06       0.59571097 0.2553047 ]        [0 0 1]        [0.34869766 0.29097286 0.3603295 ]        3\n","[0.67017484 0.07       0.59571097 0.2553047 ]        [0 0 1]        [0.34932953 0.2916674  0.35900304]        3\n","[0.67017484 0.08       0.59571097 0.2553047 ]        [0 0 1]        [0.3499599  0.29236138 0.35767874]        3\n","[0.67017484 0.09       0.59571097 0.2553047 ]        [0 0 1]        [0.3505887  0.29305473 0.35635656]        3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O-OkcVWcXfw_","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}